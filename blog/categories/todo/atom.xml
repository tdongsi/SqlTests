<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Todo | Personal Interview Notes]]></title>
  <link href="http://tdongsi.github.io/SqlTests/blog/categories/todo/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/SqlTests/"/>
  <updated>2016-09-13T23:43:22-07:00</updated>
  <id>http://tdongsi.github.io/SqlTests/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[System Design Questions]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/09/14/system-design-questions/"/>
    <updated>2016-09-14T02:13:01-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/09/14/system-design-questions</id>
    <content type="html"><![CDATA[<p>How to practice for System Design questions and some design questions.</p>

<!--more-->


<h3>Readings</h3>

<p>For Web Services, read &ldquo;Architecting in AWS&rdquo;: recognize scalability problems that AWS services trying to address and replace, for example, &ldquo;AWS Load Balancer&rdquo; with generic load balancer.</p>

<p>Read these to know the broad topics that are expected.</p>

<ol>
<li><a href="https://www.quora.com/What-system-design-distributed-systems-+-scalability-topics-should-I-study-in-order-to-adequately-prepared-for-a-Google-Software-Engineer-interview">Quora question</a></li>
<li><a href="https://www.linkedin.com/pulse/technical-design-interview-guide-success-joey-addona">The Technical Design Interview - A Guide to Success</a></li>
<li><a href="https://www.linkedin.com/pulse/test-design-architecture-interview-tips-success-kane-ho">Test Design &amp; Architecture Interview - Tips to success</a></li>
</ol>


<h3>Questions</h3>

<ol>
<li>Design a simple file system using OO programming. Just folder and files.</li>
<li>How to design a load balancer?</li>
<li>How to design Facebook News Feed?</li>
</ol>


<h3>Answers</h3>

<p>(1) Design a simple file system using OO programming. Just folder and files.</p>

<p>Use Composite pattern.</p>

<pre><code class="java">class FileNode {
  String name;
}

class File extends FileNode {
  long size;
}

class Folder extends FileNode {
  Collection&lt;FileNode&gt; children;
}
</code></pre>

<p>(2) How to design a load balancer?</p>

<p>Simple: hash and assign random. What are pros and cons?</p>

<p>(3) Design Facebook News Feed.
From <a href="https://www.reddit.com/r/cscareerquestions/comments/4ytbz3/design_facebook_news_feed_my_solution_to_the/">here</a>:</p>

<p>First some numbers to get the scale of the problem:
number of users: 109
number of users during a peak hour (upper bound): 108
number of posts during a peak hour: 106
number of other activities during a peak hour (likes, comments, saves): 1010
almost all users have less than 103 friends
The News Feed is constructed mainly based on the activity of user&rsquo;s important Facebook friends. An important friend is a user who is my friend and I have interacted with him/her at least somewhat during recent months. Interaction might include liking his/her comment, commenting on their post, chatting together, being marked on the same photo, etc. We assume the backend maintains the list of important friends. It might be updated perhaps every 60 minutes.
This ordering might be quite fuzzy. Random perturbations of this ordering might lead to users being able to rediscover friends who they have mostly stopped interacting with. The primary purpose of distinguishing important friends is to make the feed more interesting. Another advantage is reduction of hotspots: there might be people with >104 friends, but we assume that every Facebook user has at most 102 important friends.
Minimum Viable Product: The Facebook News Feed of each user is a merge of the recent posts made by all his/her important friends sorted by the score of the post. The score of the post is ideally proportional to how interesting the post is to our user. The score depends on: how old the post is (older posts are less interesting), how many likes the post received, how many likes the post received from user&rsquo;s important friends or friends, &hellip;
We primarily care about delivering an interesting News Feed. On the other hand, we don&rsquo;t really care about being able to produce an infinitely long News Feed. If our infrastructure implies that the feed is limited to 1000 items and the user cannot scroll further. There are very few legitimate uses for having infinitely scrollable feed.
Overview of the infrastructure:
Firewall
Load balancer
Front-end web servers
Memcache servers or Redis servers or something similar
Distributed database servers
Let&rsquo;s see what happens when the user requests his News Feed:
The request is specified by: the id of user whose feed we&rsquo;re displaying and the number N of requested posts.
The load balancer redirects the request to one of the web servers. It also decrypts the request. Within the datacenter, we only use unencrypted connections. To do the above, the load balancer keeps statistics of the numbers of requests each server is handling. Possibly, the load balancer might decide to start up a new server or schedule a shutdown of a server to save power. The web server checks if the user is authenticated. This is done by retrieving user&rsquo;s session data from a Google BigTable. If not logged in, s/he is redirected to the login page. If N is too large and not coming from a verified computer (like Facebook&rsquo;s API server), we reject the request and log information about a suspicious request.
The web server queries memcache for the list of important friends of the user in question.
Almost always, memcache will have this list ready in memory for all users who are currently logged in (after login, we immediately prefetch this data into memcache). The peak number of users is 108, the number of important users at most 102, each user is represented by an 8 byte identifier. This is an upper bound of 1011 bytes, which is 100 GB of memory. Even with significant overhead, room for growth and a safety factor for situations when an unexpectedly large number of users logs in at the same time (e.g., when presidential election results are announced), this can still be stored in operating memory of a single server. A distributed memcache implementation is not going to have a problem here. For each important friend, the web server sends a request for this friend&rsquo;s Activity List. Activity List is a list of his posts, likes, comments, uploaded photos, instances of being marked on a photo, etc. Each item contains a timestamp (32 bits), item type (post, comment, share, &hellip;), id of the item (e.g., the id of the post or comment), the destination id (for example, the id of the post on which the comment was made) and privacy setting (1 byte). This is 22 bytes in total. Only identifiers are stored. The data are populated at the end of the computation.
These lists are stored in a distributed database hidden behind caching servers. There is 109 users, each has at most 103 items in their Activity List. This is 22 * 1012 = 22 TB of memory. <strong> It is certainly possible to store this in a distributed file system. </strong>A distributed memcache on 128 servers each with 250 GB RAM would also handle this. Therefore, we can assume all these lists are almost always in memory.
Each item in each of these lists is assigned Relevancy Value. This depends on: The importance of the friend from whose Activity List the item is. How recent the item is. The number of likes and shares the item has (this only makes sense in the case of posts or photos). The number of likes from the user&rsquo;s other important friends. Surely, a like from 3 of my friends is more important than 10 likes from random strangers. We can access this information since we have retrieved the Activity Lists of every single important friend of the user. Since these lists include their likes and the ids of liked posts, we can specifically compute the numbers from the retrieved data. The user&rsquo;s prefered content type. Some users might like photos more than text. We remember this for each user and adjust the weight accordingly. Private posts that should not be visible to the user are removed at this point.
We sort each of these lists using the Relevancy Value and merge them.
We send this list to a content service. This service replaces all identifiers with the corresponding content (text, image links, names of users instead of user ids, &hellip;)
The web server uses a template to convert this into HTML.
There are two questions to ponder:
When the News Feed is requested again in the future, do we recalculate it from scratch? We could store the sequence calculated in one of the last steps in cache and only compute the beginning of the feed (that is: we would only compute what is new). This might make almost-infinite scrolling possible in certain cases. However, we would need to handle some corner cases. For example, the list is only approximately ordered according to the timestamp.
The above described the read path. It remains to analyze the write path: what happens when a user submits a content, likes something, etc. Well, we simply add this to his Activity List. In the case of posts, images, &hellip; we also store it on a content service server.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Questions: Operating System Drills]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/09/07/questions-operating-system-drills/"/>
    <updated>2016-09-07T00:42:41-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/09/07/questions-operating-system-drills</id>
    <content type="html"><![CDATA[<p>From &ldquo;Operating System concepts&rdquo; book.</p>

<!--more-->


<p>Drill:</p>

<ol>
<li>Chapter 1: Storage device hierarchy</li>
<li>Chapter 2: System calls and Linux examples. System boot.</li>
<li>Chapter 3: Process diagram. Process state diagram. Zombie, orphan process. Practice Exercise 3.1, 3.2.</li>
</ol>


<p>Links:</p>

<ul>
<li><a href="http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory/">http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory/</a></li>
<li><a href="http://duartes.org/gustavo/blog/post/how-the-kernel-manages-your-memory/">http://duartes.org/gustavo/blog/post/how-the-kernel-manages-your-memory/</a></li>
<li><a href="http://duartes.org/gustavo/blog/post/page-cache-the-affair-between-memory-and-files/">http://duartes.org/gustavo/blog/post/page-cache-the-affair-between-memory-and-files/</a></li>
</ul>


<p>Practice exercises:</p>

<ul>
<li><a href="http://codex.cs.yale.edu/avi/os-book/OS9/practice-exer-dir/index.html">http://codex.cs.yale.edu/avi/os-book/OS9/practice-exer-dir/index.html</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tutorial: Process Synchronization]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/09/06/tutorial-process-synchronization/"/>
    <updated>2016-09-06T00:35:03-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/09/06/tutorial-process-synchronization</id>
    <content type="html"><![CDATA[<p>Summary of chapter 5 of &ldquo;Operating System concepts&rdquo; (Dinosaur book).
Topics in this chapter are the most intensive and frequently asked during interviews.</p>

<!--more-->


<p>This chapter discuss how to prevent concurrent access to shared data that may result in data inconsistency.</p>

<h3>5.1 &amp; 5.2: Critical section</h3>

<p>Consider the producer–consumer problem, which is representative of
operating systems. Specifically, in Section 3.4.1, we described how a bounded
buffer could be used to enable processes to share memory.</p>

<pre><code class="cpp Producer process">while (true ) {

  /* produce an item in next produced */
  while (counter == BUFFER SIZE )
    ; /* do nothing */

  buffer[in] = next produced;
  in = (in + 1) % BUFFER SIZE ;
  counter++;
}
</code></pre>

<pre><code class="cpp Consumer process">while (true ) {
  while (counter == 0)
    ; /* do nothing */

  next consumed = buffer[out];
  out = (out + 1) % BUFFER SIZE ;
  counter--;
  /* consume the item in next consumed */
}
</code></pre>

<p>The above code works incorrectly in multi-thread or process situation, due to parallel modifications to &ldquo;counter&rdquo;.
When several processes access and manipulate the same data concurrently and the
outcome of the execution depends on the particular order in which the access
takes place, is called a race condition.</p>

<p>Critical section: No two processes are executing in their critical sections at the same time.
Solution to a critical section problem requires:
Mutual exclusion: only one process in CS at a time.
Progress: Selection process should not be postponed indefinitely.
Bounded waiting: once a process request to enter, waiting time is bounded.</p>

<p>Nonpreemptive kernel does not allow a process running in kernel mode to be
preempted; a kernel-mode process will run until it exits kernel mode, blocks,
or voluntarily yields control of the CPU. A nonpreemptive kernel is essentially free from race conditions.
A preemptive kernel may be more responsive, since there is less risk that a
kernel-mode process will run for an arbitrarily long period.</p>

<h3>5.3: Peterson&rsquo;s algorithm</h3>

<pre><code class="cpp Peterson's algorithm">flag[i] = true
turn = j

while ( flag[j] &amp;&amp; turn == j ) {
}

// start of CS

// end of CS
flag[i] = false flag[j] = true
turn = i

while( flag[i] &amp;&amp; turn == i ) {
}

// start of CS

// end of CS
flag[j] = false
</code></pre>

<p>NOTE: <a href="http://en.wikipedia.org/wiki/Peterson's_algorithm">Peterson’s algorithm</a> is restricted to two processes.
Filter algorithm: Peterson&rsquo;s algorithm for N processes
The filter algorithm generalizes Peterson&rsquo;s algorithm for N processes. It uses N different levels - each represents another &lsquo;waiting room&rsquo;, before the critical section. Each level will allow at least one process to advance, while keeping one process in waiting.</p>

<p><a href="http://cs.stackexchange.com/questions/12621/understanding-peterson-s-and-dekker-s-algorithms">http://cs.stackexchange.com/questions/12621/understanding-peterson-s-and-dekker-s-algorithms</a>
Both processes indicates if the other want to enter CS, it can proceed. If both processes enter at the same time, turn will be set to i and j at the same time, and only one will last.
Proof of bounded waiting: Pi will enter the CS after at most one entry by Pj.</p>

<pre><code class="plain Analogies of Peterson's algorithm">Peterson's: "I want to enter."                 flag[0]=true;
            "You can enter next."              turn=1;
            "If you want to enter and          while(flag[1]==true&amp;&amp;turn==1){
            it's your turn I'll wait."         }
            Else: Enter CS!                    // CS
            "I don't want to enter any more."  flag[0]=false;

Dekker's:   "I want to enter."                 flag[0]=true;
            "If you want to enter              while(flag[1]==true){
             and if it's your turn               if(turn!=0){
             I don't want to enter any more."      flag[0]=false;
            "If it's your turn                     while(turn!=0){
             I'll wait."                           }
            "I want to enter."                     flag[0]=true;
                                                 }
                                               }
            Enter CS!                          // CS
            "You can enter next."              turn=1;
            "I don't want to enter any more."  flag[0]=false;
</code></pre>

<h3>5.4: Sync using hardware</h3>

<p>Protect critical section by locking.</p>

<p>Many modern computer systems therefore provide special hardware
instructions that allow us either to test and modify the content of a word or
to swap the contents of two words atomically—that is, as one uninterruptible unit.</p>

<p>Atomic test_and_set() and compare_and_swap() for locking:</p>

<p>boolean test_and_set(boolean <em>target) {
boolean rv = </em>target;
*target = true;
return rv;
}</p>

<p>int compare_and_swap(int <em>value, int expected, int new value) {
int temp = </em>value;
if (<em>value == expected)
    </em>value = new value;
return temp;
}</p>

<p>Simple Mutex with atomic test_and_set()
Figure 5.5</p>

<p>Bounded-Waiting mutex with atomic test_and_set(): data structure and algorithm
Figure 5.7</p>

<h3>5.5: Mutex locks</h3>

<p>We use the mutex to lock to protect critical regions and thus prevent race conditions.</p>

<p>Calls to either acquire() or release() must be performed atomically.</p>

<p>acquire() {
while (!available)
; /<em> busy wait </em>/
available = false;
}</p>

<p>release() {
available = true;
}
Usage:</p>

<p>acquire()</p>

<p>// start of CS</p>

<p>// end of CS</p>

<p>release()</p>

<p>The main disadvantage of the implementation given here is that it requires busy waiting. This type of mutex lock is also called a spinlock.
Spinlocks do have an advantage, however, in that no context switch is required.</p>

<h3>5.6: Semaphores</h3>

<p>A semaphore S is an integer variable that, apart from initialization, is
accessed only through two standard atomic operations: wait() and signal().</p>

<p>wait(S) {
while (S &lt;= 0 )
; // busy wait
S&ndash;;
}</p>

<p>signal(S) {
S++;
}</p>

<p>The value of a counting semaphore can range over an unrestricted domain. Versus binary semaphore, which is similar to mutex.
Counting semaphores can be used to control access to a given resources of a finite number of instances.</p>

<p>We can also use semaphores to solve various synchronization problems.
For example,consider two concurrently running processes: P1 with a statement
S1 and P2 with a statement S2 . Suppose we require that S2 be executed only
after S1 has completed. We can implement this scheme readily by letting P1
and P2 share a common semaphore synch, initialized to 0. In process P1 , we
insert the statements
S1;
signal(synch);</p>

<p>In process P2 , we insert the statements
wait(synch);
S 2 ;</p>

<p>Because synch is initialized to 0, P2 will execute S2 only after P1 has invoked
signal(synch) , which is after statement S1 has been executed.</p>

<p>Deadlock:
P 0 P 1
wait(S); wait(Q);
wait(Q); wait(S);
. .
. .
. .
signal(S); signal(Q);
signal(Q); signal(S);</p>

<p>Priority inversion:
The problem of priority inversion is when three processes of different priorities L &lt; M &lt; H. H is waiting for L to finish with a certain resource. M process becomes runnable and preempts L. Indirectly, process M with lower priority affects how long process H must wait for resource.
It occurs when the system has more than two priorities. However, it is almost always the case.
Solution: priority-inheritance protocol: all processes that use a resource, waited by a higher priority process, will inherit the highest priority until they are done with the resource.</p>

<h4>Semaphore implementation</h4>

<p>The naive definition of wait() and signal() above presents the same problem of busy waiting.
In actual implementation, when a process execute wait() operation and find that semaphore value is not positive, it must wait. However, instead of busy waiting, the process block itself.
In this implementation, semaphore values may be negative, while they are never negative in classical definition with busy waiting. If a semaphore value is negative, its magnitude indicates the number of waiting processes (note different order of decrement in wait()).</p>

<p>It is critical that semaphore operations be executed atomically: no two processes can execute wait() and signal() operations on the same semaphore at the same time.
In a multiprocessor environment, usually compare_and_swap() or spin locks are used to ensure wait() and signal() are atomic.
So, we admit that busy waiting is NOT eliminated in this implementation. However, busy waiting is limited to CS of the wait() and signal() operations. These CSs are short (about 10 instructions). Thus, CS is almost never occupied, and busy waiting is rare and short, if ever happens.</p>

<h3>5.7: Classic Problems of Synchronization</h3>

<p>Use semaphores for synchronization. Actual implementation can use mutex instead of binary semaphore.</p>

<p>Bounded-Buffer (Consumer-Producer) problem
Problem: See 5.1.
Solution: The producer and consumer share the following data structure:
The mutex is used to provide mutual exclusion for accesses to the buffer pool.
int n;
semaphore mutex = 1;
semaphore empty = n;
semaphore full = 0</p>

<p>Producer    Consumer
do {
&hellip;
/<em> produce an item in next produced </em>/
&hellip;
wait(empty);
wait(mutex);
&hellip;
/<em> add next produced to the buffer </em>/
&hellip;
signal(mutex);
signal(full);
} while (true);
do {
wait(full);
wait(mutex);
&hellip;
/<em> remove an item from buffer to next consumed </em>/
&hellip;
signal(mutex);
signal(empty);
&hellip;
/<em> consume the item in next consumed </em>/
&hellip;
} while (true);</p>

<p>Reader-Writer problem:
Writers should have exclusive access while writing to the shared database.
There are several variants of “reader-writer” problems:
First problem: No reader to be kept waiting unless a writer has already obtained accesss.
Second problem: Once writer is ready, that writer perform its write ASAP. No new readers may start reading.
A solution to either problem may result in starvation.</p>

<p>Solution to first reader-writer problem: shared data structure
semaphore rw_utex = 1;
semaphore mutex = 1;
int read count = 0;
The mutex semaphore is used to ensure mutual exclusion when the variable read count is updated.
The read count variable keeps track of how many processes are currently reading the object.
The semaphore rw_mutex functions as a mutual exclusion semaphore for the writers.</p>

<p>Writer  Reader
do {
wait(rw mutex);
&hellip;
/<em> writing is performed </em>/
&hellip;
signal(rw mutex);
} while (true);
do {
wait(mutex);
read count++;
if (read count == 1)
wait(rw mutex);
signal(mutex);
&hellip;
/<em> reading is performed </em>/
&hellip;
wait(mutex);
read count&ndash;;
if (read count == 0)
signal(rw mutex);
signal(mutex);
} while (true);</p>

<p>Dining Philosopher problem: This solution can create a deadlock
semaphore chopstick[5];</p>

<p>do {
wait(chopstick[i]);
wait(chopstick[(i+1) % 5]);
&hellip;
/<em> eat for awhile </em>/
&hellip;
signal(chopstick[i]);
signal(chopstick[(i+1) % 5]);
&hellip;
/<em> think for awhile </em>/
&hellip;
} while (true);</p>

<p>Several possible remedies to the deadlock problem are replaced by:
• Allow at most four philosophers to be sitting simultaneously at the table.
• Allow a philosopher to pick up her chopsticks only if both chopsticks are
available (to do this, she must pick them up in a critical section).
• Use an asymmetric solution—that is,an odd-numbered philosopher picks
up first her left chopstick and then her right chopstick, whereas an even-
numbered philosopher picks up her right chopstick and then her left
chopstick.</p>

<p>In 5.8, we use monitor (equivalent to a waiter to tell which philosopher should eat) to provides deadlock-free solution.
A deadlock-free solution does not necessarily eliminate the possibility of starvation.</p>

<h3>5.8: Monitors</h3>

<p>Semaphore is not a complete solution. If a single process is not well-behaved (semaphore used incorrectly), the system break down.
Incorrect order of signal() and wait(): mutual exclusion is no longer guaranteed.
wait() is used in place of signal(): a deadlock may occur.
wait() or signal() or both are omitted: mutual exclusion violated or deadlock.</p>

<p>Syntax of a monitor:</p>

<p>Local variables of a monitor can be accessed by only the local functions. Only one process at a time is active within the monitor.
We also defines condition construct: condition x, y; // condition variables
The only operations that can be invoked on a condition variable are wait() and signal().
The operation x.wait(); means that the process invoking this operation is suspended until another process invokes x.signal();
The x.signal() operation resumes exactly one suspended process. If no process is suspended, then the signal() operation has no effect.
(Different from semaphore’s signal(): semaphore() signal always change the state of semaphore, condition’s signal() may not).</p>

<p>Dining Philosophers solution using Monitors:
Monitor is acting like a waiter/moderator. Before a philosopher starts eating, she informs the waiter (invoked operation pickup()) and the waiter will tell her what to do.
After she is done eating, she again informs the waiter (putdown()). It is still possible that some philosopher will starve to death.</p>

<p>condition self[5];
allows philosopher i to delay herself when she is hungry but is unable to obtain the chopsticks she needs.</p>

<p>Implement a Monitor with Semaphores
Check Section 5.8.3 page 229.</p>

<p>Resuming Processes within a Monitor
One simple solution is to use FIFO ordering.
Another solution is conditional-wait construct, with c is the priority number input.
x.wait&copy;;
When x.signal() is executed, the process with the smallest priority number is resumed next.</p>

<p>Java monitors
Java uses monitor for thread synchronization.
Every object in Java has a single lock associated with it. When a method is declared synchronized, calling the method requires owning the lock of the object.
If the lock is not available, the synchronized method is placed in the entry set for the object’s lock.
The Java Object class’s method wait() and notify() are similar to wait() and signal() statements for a monitor.</p>
]]></content>
  </entry>
  
</feed>
