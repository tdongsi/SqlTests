<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Database | Personal Interview Notes]]></title>
  <link href="http://tdongsi.github.io/SqlTests/blog/categories/database/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/SqlTests/"/>
  <updated>2016-09-03T02:06:56-07:00</updated>
  <id>http://tdongsi.github.io/SqlTests/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tutorial: Dashboard for Business Analytics]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/09/16/tutorial-dashboard-for-business-analytics/"/>
    <updated>2016-09-16T15:40:35-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/09/16/tutorial-dashboard-for-business-analytics</id>
    <content type="html"><![CDATA[<p>From &ldquo;Pluralsight: Business Dashboard Fundamentals&rdquo;.</p>

<!--more-->


<p>== Module  1 ==
Ppl looks for different things in data</p>

<pre><code>* Trend: bar, graph
* Average
</code></pre>

<p>Above all else, show data.
Data-pixel ratio: data pixels/non-data pixels.
How to enhance data-pixel ratio:</p>

<pre><code>* Granularity: depends on the question you want to answer
* 
    * monthly if you want to know monthly sales, daily if you want to know what happens last Tuesday.
    * category or sub-category: you can have category with drill down function.

* Annotation: similar to Granularity. Minimize it to enough to answer the question.
</code></pre>

<p>Tricks &amp; Tips:</p>

<pre><code>* Plots (over Bars): plot show the trends for different components over time.
* Sizing Bars: Preserve True Portions: starting Y from 0.
* 
    * Sizing Bars: shows proportions, relative progression.

* Scatter Plots: show clusters, outliers.
* Radio displays: usually a bad idea. Waste of space, hard to discern between slices.
</code></pre>

<p>== Module 2: Common Charts ==</p>

<p>Basic data Presentation Methods - Chart Types</p>

<pre><code>* Geo-Spatial - Maps: anything related to geographic distribution, i.e., when geography matters. e.g. real estates, oil industry.
* Correlation - Scatter Plots: two measurements (e.g., sales to profit).
* Hierarchical - Drill down Tree: data is hierarchical: Category -&gt; Subcateogries.
* Categorical - Bar Charts: comparing categories (sales by region)
* Time Series - Line Charts: progression over time. (sales by month)
* 
    * NOT: Area Charts.

* Distribution - Histograms: Trying to answer what is “normal”. e.g., home prices, salaries.
</code></pre>

<p>Others</p>

<pre><code>* Box plot: distribution, percentiles, median in 1 chart.
* Bullet graph: actuals to target. Invented by Stephen Few.
* 
    * Dark bar is actual, reference line is target. Color code bands are average, good, bad target range.
    * https://en.wikipedia.org/wiki/Bullet_graph

* Sparkline: Multiple line charts. Best used for monitoring dashboard.
* Heat map: Large combinations of dimensions. Color is everything here.
</code></pre>

<p>Charts to avoid</p>

<pre><code>* Pie charts: angles make it hard to compare. Usually decorative, not informative. Space is wasted.
* Polar charts
* Stacked area charts:
* 
    * Only tell the story of the bottom line and the total. Anything in between, you can’t really tell if they are growing or not.
    * Misleading/Confusing: is the top the total or another category?
</code></pre>

<p>== Module 3: Dashboard planning ==</p>

<p>Steps:</p>

<pre><code>* User Request
* Prioritization
* Planning
* Design
* Development
* Delivery to User
</code></pre>

<p>== Module 4: Dashboard design ==</p>

<p>Audience is King. Know your audience.</p>

<pre><code>* Who is using it? 
* Are they technical or prefer dumbed down answers? Are they intimate with data?
* What is primary objective? What questions that they try to answer? What questions this dashboard MUST answer?
* What impact of the answer? How will they use metrics? (Role, what decisions they make)
* When will the dashboard is used? (Weekly? Daily?) Dashboard is exploratory or explanatory?
* What level of confidence in data sources?
</code></pre>

<p>Dashboard layout: F layout is the most natural for web/desktop viewing.</p>

<p>== Module 5 &amp; 6: Tableau ==</p>

<p>Connecting to Data:
You can connect to Excel, text file (csv or tab), or HP Vertica.
You can specify data import like Excel or using Custom SQL.
After importing, Tableau may import all data into its own internal data engine (with compression, data reorganization easier for analytics).
It also divides data into dimensions and measures.
Dimensions are further categorized into: geographic (e.g., region, postal code), number, text, date (e.g., calendar, order_date).
Facts are usually numbers but it can be other categories: e.g., geographic for latitude/longitude measures.</p>

<p>Dimensions: context for analysis
Facts (Measures):</p>

<p>Visualizing data:
Tableau has “Show Me” button that gives suggestions for different combinations of dimensions and fact data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tutorial: Dimensional Modelling]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/09/15/tutorial-dimensional-modelling/"/>
    <updated>2016-09-15T15:38:20-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/09/15/tutorial-dimensional-modelling</id>
    <content type="html"><![CDATA[<p>From Kimball group reader.</p>

<!--more-->


<p>Design items:
You need to do data profiling to keep data processed to min. One way to profile data changes is to use CDC column.
Check CDC columns: e.g. last_update_ts. If CDC columns are not available, work with production system DBA to add one.
Otherwise, check application log file/Message traffic.</p>

<p>Item 1.5
Divide data into dimension and fact tables: 90% of the time the division is obvious.</p>

<pre><code>* Dimensions: static entities in the environment
* 
    * Text descriptions are obvious data going into dimension

* Facts: numeric observations/measurements.
* 
    * Unpredictable, numeric numbers are the usual suspects.
</code></pre>

<p>Grain of fact table = a measurement in physical, real-world.</p>

<p>Design steps:</p>

<pre><code>* Determine the single physical event you want to record -&gt; fact table. Other details will follow in dimension tables.
* 
    * What event is a single row in fact table representing? E.g. for fact_sale_event, the grain is literally the beep of the scanner.

* Strive to make facts additive.
* 
    * E.g.: Sale event can go into fact table as (price, unit), but the information (sale amount, unit) contains the same information but better since sale amount (aka extended price) = price * unit.

* Some data can be in both. The goal is ease of use, not methodology correctness.
* 
    * E.g.: Coverage amount of insurance policies can be in both dim_coverage and fact_sale_event. 
</code></pre>

<p>Item 1.6
Bus matrix to communicate/manage dimension tables.</p>

<p>TODO: Table of bus matrix</p>

<p>Item 1.8 Slow Changing Dimensions
Type 0: Constant. Ignore changes.
Type 1: Simple overwrite (best used for error correction).
Type 2: Create another row and save history.
The standard implementation is: surrogate key (PK), durable ID, … attributes …, effective_start_date, effective_end_date, change_reason, current_flag.</p>

<p>Type 3: Create another column for alternate info.</p>

<p>Item 1.10. Fact tables
Data warehouse is built on fact tables expressed at the lowest possible grain.
Higher grain aggregated tables such as category sales by district.
Three kinds of fact tables:</p>

<pre><code>1. Transaction Grain: corresponds to a measurement taken at a single instant.
    1. Unpredictably sparse or dense.
    2. Can be enormous. Cannot guarantee all possible foreign keys represented.
    3. E.g.: fact_qbo_subscription_event

2. Periodic Snapshot Grain: corresponds to a predefined span of time.
    1. Predictably dense.
    2. Can be large even there is no activity.
    3. E.g.: Account balance for an account at some time.

3. Accumulating Snapshot Grain.
    1. Fact entries are overwritten and udpated.
    2. E.g.: Order processing
</code></pre>

<p>Surrogate Keys (integer key, assigned in sequence) are recommended for Fact tables.
In Vertica, CREATE SEQUENCE.</p>

<p>Item 4.1
Too smart interviewers make it harder to extract requirements from business:</p>

<pre><code>* Long-winded questions
* Even worse, some questions box the interviewee into a corner because of some bias. And the interviewees do not know how to get out.
* 
    * Just ask and listen. Let them guide you step by step.
</code></pre>

<p>Item 5.1-5.3: Compare normalized modeling (3NF) vs dimensional modeling (DM)
Why dimensional modeling over normalized modeling?</p>

<pre><code>1. Normalized modeling is intended for transactional databases, making update and delete efficient. It’s not needed in BI/DW.
2. Normalized modeling for a complex business process will result in a very large ER diagram (similar to US cities-freeway maps). Business users cannot simply use that diagram to query what they need to know.
    1. The result ER is usually overwhelming and cannot be viewed in its entirety.
    2. E.g.: How to drive from SJ to NY? Maybe going to Sacramento through 580, then to Salt Lake City, and then what? Similarly, you need to know which table to join.
    3. In the same analogy, it’s actually worse to join the tables since the tables are not static, they are moving cities.
</code></pre>

<p>Dimensional Modeling: top-down design process.</p>

<pre><code>* each fact table represents a business process.
* Support two operations: browse and multi-table joins.
* It is important to keep the dimension tables flat, without being normalized into snowflake structure.
</code></pre>

<h3>Questions</h3>

<p><a href="http://learndatamodeling.com/blog/data-modeling-interview-questions/">http://learndatamodeling.com/blog/data-modeling-interview-questions/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Database Question Bank]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/09/12/database-question-bank/"/>
    <updated>2016-09-12T01:41:54-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/09/12/database-question-bank</id>
    <content type="html"><![CDATA[<p>Questions about database in general. For data warehouse and dimensional modelling, check <a href="/blog/2016/09/15/tutorial-dimensional-modelling/">this</a>.</p>

<!--more-->


<h3>Relational Database topics</h3>

<p>Basics</p>

<ul>
<li>SELECTing columns from a table</li>
<li>Aggregates Part 1: COUNT, SUM, MAX/MIN</li>
<li>Aggregates Part 2: DISTINCT, GROUP BY, HAVING</li>
</ul>


<p>Intermediate</p>

<ul>
<li>JOINs, ANSI-89 and ANSI-92 syntax</li>
<li>UNION vs UNION ALL</li>
<li>NULL handling: COALESCE &amp; Native NULL handling</li>
<li>Subqueries: IN, EXISTS, and inline views</li>
<li>Subqueries: Correlated</li>
<li>WITH syntax: Subquery Factoring/CTE</li>
<li>Views</li>
</ul>


<p>Advanced Topics</p>

<ul>
<li>Functions, Stored Procedures, Packages</li>
<li>Pivoting data: CASE &amp; PIVOT syntax</li>
<li>Hierarchical Queries</li>
<li>Cursors: Implicit and Explicit</li>
<li>Triggers</li>
<li>Dynamic SQL</li>
<li>Materialized Views</li>
<li>Query Optimization: Indexes</li>
<li>Query Optimization: Explain Plans</li>
<li>Query Optimization: Profiling</li>
<li>Data Modelling: Normal Forms, 1 through 3</li>
<li>Data Modelling: Primary &amp; Foreign Keys</li>
<li>Data Modelling: Table Constraints</li>
<li>Data Modelling: Link/Corrollary Tables</li>
<li>Full Text Searching</li>
<li>XML</li>
<li>Isolation Levels</li>
<li>Entity Relationship Diagrams (ERDs), Logical and Physical</li>
<li>Transactions: COMMIT, ROLLBACK, Error Handling</li>
</ul>


<h4>References</h4>

<ul>
<li><a href="http://stackoverflow.com/questions/2119859/questions-every-good-database-sql-developer-should-be-able-to-answer">http://stackoverflow.com/questions/2119859/questions-every-good-database-sql-developer-should-be-able-to-answer</a></li>
<li><a href="http://www.careercup.com/page?pid=database-interview-questions">http://www.careercup.com/page?pid=database-interview-questions</a></li>
</ul>


<h3>Questions</h3>

<p>(1) Given these two databases:</p>

<pre><code class="plain Given databases">id name id name 
-- ---- -- ---- 
1 Pirate 1 Rutabaga 
2 Monkey 2 Pirate 
3 Ninja 3 Darth Vader 
4 Spaghetti 4 Ninja
</code></pre>

<p>Explain the following JOINs:</p>

<ol>
<li>INNER JOIN</li>
<li>LEFT and RIGHT OUTER JOIN</li>
<li>FULL JOIN</li>
<li>CROSS JOIN</li>
</ol>


<p>B set:</p>

<ol>
<li>What is normalization and why is it important?</li>
<li>What are some situations where you would de-normalize data?</li>
<li>What is a transaction and why is it important?</li>
<li>What is referential integrity and why is it important?</li>
<li>What steps would to take to investigate reports of slow database performance?</li>
<li>What is an index and how does it help your database?</li>
<li>If someone were to make the claim that: &ldquo;every SELECT always include DISTINCT&rdquo;; how would you comment on the claim?
OLTP and OLAP points of view?</li>
</ol>


<h3>Answer keys</h3>

<h4>Section A</h4>

<p>(1) <a href="http://www.codinghorror.com/blog/2007/10/a-visual-explanation-of-sql-joins.html">http://www.codinghorror.com/blog/2007/10/a-visual-explanation-of-sql-joins.html</a></p>

<h3>Section B</h3>

<p>(1) Normalizing: remove redundancy -> remove update and delete anomaly -> more efficient data storage and consistent data.</p>

<p>Summary from Introduction to Databases
To reduce redundancy. From redundancy, it will lead to update and delete anomaly.
E.g.: dim_company: region/country info will be repeated -> redundancy.
E.g.: North Korea and South Korea merged into Korea -> delete those regions will delete companies -> delete anomaly.</p>

<p>Popular normal forms:</p>

<p>BCNF: For each functional dependency A-> B, A is key.
Functional dependency: A -> B: same A leads to same B. E.g.: ID -> name.</p>

<p>4NF: Functional dependency + Multivalued dependency.
Multivalued dependency: A ->> B, C: each A lead to all combo (B x C). E.g.: ID -> region x language
4NF: For each multivalued dependency A ->> B, C, A is key. I.e.: (A, B, C) is decomposed to (A, B) and (A, C).</p>

<p>(2) When there is no data update or deletion. For example: data warehouse situations.
Analytical processing: joining multiple tables is not efficient. The SQL queries are hard to write.</p>

<p>(3)Why is transaction is important:</p>

<ul>
<li>Concurrency: Ensure consistent data read/write while providing concurrent data access.</li>
<li>Failure-tolerance: Resilience to system failures.</li>
</ul>


<p>A transaction is a sequence of SQL statements treated as a unit. The effect of a transaction is either full or none at all.
Transactions appear to run in isolation.</p>

<p>ACID: atomic, consistency, isolation, durability</p>

<p>(4) You have a foreign key, reference to another table.
When that key is deleted from the other table, referential integrity is compromised.</p>

<p>(5) Check current state: hang processes, long running queries.
Optimize SQL queries.
Indexes.</p>

<p>(6) What is it: persistent data structure, stored in database.
Purpose: improve data lookup performance.
Instead of scanning a whole relational table for a record, using the index, the location of a record can be returned almost immediately.
Implementation: Balanced trees (B tree, B+ tree), Hash Map (for equality condition only)</p>

<p>A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.
An index is a copy of select columns of data from a table that can be searched very efficiently that also includes a low-level disk block address or direct link to the complete row of data it was copied from.</p>

<p>(7) OLTP: Suppose your query is correct, and does not return any duplicates, then including DISTINCT simply forces the RDBMS to check your result (zero benefit, and a lot of additional processing).
Suppose your query is incorrect, and does return duplicates, then including DISTINCT simply hides the problem (again with additional processing).
It would be better to spot the problem and fix your query.
It&rsquo;ll run faster that way.</p>
]]></content>
  </entry>
  
</feed>
