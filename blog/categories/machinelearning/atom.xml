<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machinelearning | Personal Interview Notes]]></title>
  <link href="http://tdongsi.github.io/SqlTests/blog/categories/machinelearning/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/SqlTests/"/>
  <updated>2018-03-10T01:01:25-08:00</updated>
  <id>http://tdongsi.github.io/SqlTests/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Anomaly Detection]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2021/11/02/anomaly-detection/"/>
    <updated>2021-11-02T22:42:54-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2021/11/02/anomaly-detection</id>
    <content type="html"><![CDATA[<p>Anomaly detection is pretty important in DevOps world.
It is great if it can generate some alarm when something odd is happening in monitoring metrics.
This post will discuss some basic ideas of how to approach Anomaly Detection.</p>

<!--more-->


<h3>Anomaly Detection</h3>

<p>According to <a href="/download/microsoft-machine-learning-algorithm-cheat-sheet-v6.pdf">this cheat sheet</a>, the standard approaches are PCA-based anomaly detection and one-class SVM (>100 features, aggressive boundary).
These are rules of thumb: for specific sets of data with specific advanced information can lead to another more efficient approach.</p>

<p>An analogy: without specific information, merge sort is the safest choice for sorting.
However, with extra information about incoming data such as its randomness (quicksort for really random data) or range of possible values (radix sort for range much smaller than numbers), you can find a better choice for sorting.</p>

<h3>Principle Component Analysis (PCA)</h3>

<h4>Theory</h4>

<h4>Application to Anomaly Detection</h4>

<h3>Support Vector Machine (SVM)</h3>

<p>General theory: The most common theory is two-class SVM where we find the hyperplane that best divides the samples of two classes.
The problem can be formulated as a constrained optimization problem, which can be solved by quadratic programming methods.
At the end, there are some samples that are closest to the optimal hyperplane is called support vectors.
The awesome thing about SVM is that you can apply non-linear transformations, including adding more dimensions, to samples in both classes and SVM still works.
Such transformations (kernels) must have some properties and there are list of common kernel types.</p>

<p>The error is bounded by the number of support vectors -> it is better to have low average support vector.</p>

<ul>
<li><a href="https://www.youtube.com/watch?v=eHsErlPJWUU">Tutorial with familar notation (Caltech)</a></li>
<li><a href="https://www.youtube.com/watch?v=_PwhiWxHK8o">MIT tutorial</a></li>
</ul>


<p>One class SVM: samples are from positive class only.</p>

<ul>
<li><a href="https://www.youtube.com/watch?v=rNGtj2iEw6g">One class SVM tutorial</a></li>
</ul>


<h4>Application to DevOps</h4>

<ul>
<li><a href="https://www.youtube.com/watch?v=5vrY4RbeWkM">Mostly Gausian + Correlation for Context data</a></li>
</ul>


<h3>Reference</h3>

<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-cheat-sheet">Machine Learning cheat sheet</a></li>
<li><a href="/download/microsoft-machine-learning-algorithm-cheat-sheet-v6.pdf">PDF mirror</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
