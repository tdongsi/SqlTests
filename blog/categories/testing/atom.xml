<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Testing | Personal Interview Notes]]></title>
  <link href="http://tdongsi.github.io/SqlTests/blog/categories/testing/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/SqlTests/"/>
  <updated>2017-04-29T23:56:48-07:00</updated>
  <id>http://tdongsi.github.io/SqlTests/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[(Pt. 1) Functional Testing for Data Marts]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/12/02/misc-functional-testing-for-data-marts/"/>
    <updated>2016-12-02T22:45:38-08:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/12/02/misc-functional-testing-for-data-marts</id>
    <content type="html"><![CDATA[<p>For overview, see <a href="/blog/2016/03/16/sql-unit-overview/">here</a>.</p>

<p>In this blog post, I will go over on different approaches over time to verify if a data mart or data warehouse is implemented correctly, and pros and cons associated with each approach.</p>

<h3>Level 0: Manual testing</h3>

<ul>
<li><strong>Pros</strong>:

<ol>
<li>Easy to get started.</li>
</ol>
</li>
<li><strong>Cons</strong>:

<ol>
<li>Time consuming for many tests with multiple runs.</li>
<li>Not repeatable.</li>
</ol>
</li>
</ul>


<p>Early on in Big Data projects, there was not much automation.
Big Data projects are much different from typical software projects: most of the code complexity (and bugs) lies in Extract-Transform-Load (ETL) processes, typically implemented as a number of SQL scripts.
There are not many tools available for automated testing of SQL scripts, especially for Vertica.</p>

<p>At the beginning, quality engineers and data analysts tested data marts by using a number of SQL queries as test queries.
Data analysts are <em>de facto</em> end-users and main testers and many of those test queries are based on their experience.</p>

<p><img class="center" src="/images/sql/squirrel.png" title="Manual testing" ></p>

<p>We used some SQL clients such as SQuirreL as shown above, connected to Vertica using some JDBC driver, ran the test queries and verified that the outputs match our expectations.
This process is pretty much manual. If an ETL is updated <code>n</code> times, we have to repeat this <code>n</code> times.
Most of the test queries can only tests the <strong>end results</strong> of ETL processes, where data analysts have domain knowledge on: they know what numbers from those final views or tables should look like.
If there are multiple steps (multiple SQL scripts) in those ETL processes, the intermediate tables are not really accessible to data analysts.
Sometimes, some of these tests are pretty heuristic and arbitrary: e.g., this number of products sold in some channel is &ldquo;unusually&rdquo; high today, which &ldquo;seems&rdquo; to indicate that ETL went wrong in some intermediate step.</p>

<!--
Functions is not common. 
-->


<h3>Level 1: TestNG</h3>

<ul>
<li><strong>Pros</strong>:

<ol>
<li>Automated, repeatable. Run multiple times with minimal additional effort.</li>
</ol>
</li>
<li><strong>Cons</strong>:

<ol>
<li>Java and SQL codes are cluttered together.</li>
<li>Hard to read, hard to maintain.</li>
</ol>
</li>
</ul>


<p>After some rounds of manual testing, we started looking into automating the process.
Similar to manual testing, the test cases should be in SQL, to be executed against the data marts for validation.
The only difference is that it is up to the QEs to organize and automate the execution of those SQL test queries.
Since the test queries can be sent over a JDBC client like SQuirreL, we can do those programmatically as TestNG test cases.
The test SQL queries, defined as Java strings in TestNG test cases, are sent to the data marts through their respective JDBC interface for execution.</p>

<pre><code class="java Test query as constant Java string">public static final int DIM_REGION_COUNT = 245;

@Test(enabled = true)
public void validate_dim_region() {
        // First test query
        String query = "select count(*) from dim_region";
        int output = getJdbcConnection().executeQuery(query);
        Assert.assertTrue(output == DIM_REGION_COUNT, "dim_region count:");

        // Second test query
}
</code></pre>

<p>Here, the test queries are defined as constant strings in Java.
Note that the test query above is intended to be simple to illustate the automation.
The actual test queries are usually more complex than that.
The results will be captured in JUnit/TestNG tests, and expectations are verified by using various TestNG assertions.
We also remove heuristic tests that cannot be verified using assertions.
Instead, those tests will be verified during User-Acceptance Test phase where data analysts will try out the final views of data marts.
In addition, we add tests to verify intermediate steps of the ETL processes.</p>

<p>The problem of this approach is that the SQL tests are heavily cluttered by Java codes.
This problem is getting worse when the test queries are usually more complex that they cannot fit into single lines, such as one shown below.
When the number of SQL tests grows larger, it is hard to keep track of all SQL test queries in Java source files.</p>

<pre><code class="java A complex SQL query as Java string">        String query = "WITH Total_Traffic AS\n" + 
                "(\n" + 
                "    SELECT temp.* from temp as clickstream_data\n" + 
                "    where filter_key = 1\n" + 
                ")\n" + 
                ", Rock_Music as\n" + 
                "(\n" + 
                "    select * from Total_Traffic\n" + 
                "    WHERE lower(evar28) LIKE 'rock_mus%'\n" + 
                ")\n" + 
                ", Instrumental_Music as\n" + 
                "(\n" + 
                "    select * from Total\n" + 
                "    WHERE evar28 LIKE '%[ins_mus]%'\n" + 
                ")\n" + 
                ", Defined_Traffic as\n" + 
                "(\n" + 
                "    select * from Rock_Music\n" + 
                "    UNION\n" + 
                "    select * from Instrumental_Music\n" + 
                ")\n" + 
                "select traffic_date_key\n" + 
                ", count(distinct visitor_id) as unique_visitor\n" + 
                "from Defined_Traffic\n" + 
                "group by traffic_date_key";
</code></pre>

<h3>Level 2: Properties files</h3>

<ul>
<li><strong>Pros</strong>:

<ol>
<li>Automated, repeatable. Run multiple times with minimal additional effort.</li>
<li>It is easier to manage SQL test queries. Each test has a name.</li>
</ol>
</li>
<li><strong>Cons</strong>:

<ol>
<li>Test queries and their assertions (expected ouputs) are not paired. Hard to look up and update expected outputs.</li>
<li>All queries have to be in a single line. Hard to read for long test queries.</li>
</ol>
</li>
</ul>


<p>For the next step, we tried to resolve the problem of Java and SQL codes mixed together.
In this approach, SQL tests and Java codes are partitioned, with SQL queries are contained in <code>.properties</code> files, separate from supporting Java codes in <code>.java</code> files.
The SQL test queries will be read by TestNG test cases, using key strings, before sending to database for execution.
The same example above, when organized in this approach, will be as follows:</p>

<pre><code class="java Test query in properties file">public static final int DIM_REGION_COUNT = 245;
public static final String TEST_QUERY_RESOURCE = "test_queries.properties";

@Test(enabled = true)
public void validate_dim_region() {
        // First test query
        String query = PropertyUtil.getProperty(TEST_QUERY_RESOURCE, "dim_region_count");
        int output = getJdbcConnection().executeQuery(query);
        Assert.assertTrue(output == DIM_REGION_COUNT, "dim_region count:");

        // Second test query
        query = PropertyUtil.getProperty(TEST_QUERY_RESOURCE, "dim_region_data");
}
</code></pre>

<pre><code class="properties test_queries.properties">dim_region_count=select count(*) from dim_region
dim_region_data=another test query to verify data
</code></pre>

<p>Using this approach, each test can have a name to express its purpose.
One can get an overview of the SQL test queries by simply looking at the <code>properties</code> file.
The supporting Java code that executes those test queries are abstracted into separate <code>java</code> files and can be ignored.</p>

<p>The problems of the above approach are:</p>

<p>(1) SQL test queries are really hard to read in <code>properties</code> file.
Each SQL test string must be in a single line.
Adding white spaces, such as newlines and tabs, for clarity is not possible as it will make the test query truncated and invalid.
Unfortunately, it is very common that SQL queries are long, with multiple JOIN statements, especially in data mart with <a href="https://en.wikipedia.org/wiki/Star_schema">star schema</a>.
For hundreds of test cases with complex queries like example below, it is impossible to read in <code>.properties</code> file.</p>

<pre><code class="properties complex_test_queries.properties">complex_query=WITH Total_Traffic AS ( SELECT temp.* from temp as clickstream_data where filter_key = 1), Rock_Music as ( select * from Total_Traffic WHERE lower(evar28) LIKE 'rock_mus%'), Instrumental_Music as (select * from Total WHERE evar28 LIKE '%[ins_mus]%'), Defined_Traffic as (select * from Rock_Music UNION select * from Instrumental_Music) select traffic_date_key, count(distinct visitor_id) as unique_visitor from Defined_Traffic group by traffic_date_key
</code></pre>

<p>(2) Parts of tests are still in Java (in TestNG assertions), making them hard to maintain and less accessible to data analysts.
From the <code>properties</code> file, it is not clear what is the expected output of the SQL queries.
If there is a test failure, one still has to look it up in Java codes to understand and investigate.
Many data engineers and data analysts may be not familiar with Java and TestNG enough to look for and understand failures in test cases.
It is also worth noting that most of SQL queries are expected to return zero row or integer values like 0.
For example, a common test query is to find all duplicate records, which is expected to has zero row returned.
Even those simple assertions have to be encoded using Java and TestNG&rsquo;s library methods.</p>

<h3>Level 3: Script files</h3>

<ul>
<li><strong>Pros</strong>:

<ol>
<li>Automated, repeatable. Run multiple times with minimal additional effort.</li>
<li>It is easier to maintain SQL test queries.</li>
<li>Assertions/Expected outputs are paired with test queries.</li>
<li>Readable by data analysts.</li>
</ol>
</li>
<li><strong>Cons</strong>:

<ol>
<li>Slightly more complex setup to instantiate a SQL Test Runner.</li>
<li>Slightly longer running time.</li>
</ol>
</li>
</ul>


<h4>Motivation</h4>

<p>In recent Big Data projects, I tried to explore a way to improve readability of SQL tests.
The main motivation for this &ldquo;Level 3&rdquo; is my testing philosophy: <strong>prioritize readability of tests when possible</strong>.</p>

<p>Readable tests are easier to write, automate, and maintain.
More importantly, ask yourself: If you write a software, you have tests to validate it; if you write a test, how do you validate your test?
It does not make sense to write tests for tests.
Only by making tests <strong>readable</strong>, you can verify and maintain the tests.</p>

<p>Readable tests also promote collaboration between developers, data analysts and QEs.
Readable tests can be easily shared with developers and data analysts for debugging purposes, especially when they are most comfortable in SQL.
If the tests are readable and accessible to developers, they can easily run the tests on their own, without much intervention from QEs.</p>

<h4>Implementation</h4>

<p>In this approach, I implemented a <a href="/blog/2016/03/28/sql-unit-test-runner/">test framework</a>. The same tests shown in the last sections, using that framework, will look like this:</p>

<pre><code class="java Test query in test files">private SqlTestRunner testRunner;

@Before
public void setup() {
    testRunner = new SqlTestRunner(getJdbcConnection());
}

@Test(enabled = true)
public void validate_dim_region() throws Exception {
        testRunner.runScript("testscript/dim_region.test");
}
</code></pre>

<pre><code class="plain dim_region.test">/* @Test
{
  "name" : "dim_region_count",
  "query" : "select count(*) from dim_region",
  "expected" : "245"
}
*/

/* This is a comment.
Complext test query follows.
*/

/* @Test
{
  "name" : "check_traffic",
  "query" : "WITH Total_Traffic AS
      (
          SELECT temp.* from temp as clickstream_data
          where .... -- filtering
      )
      , Rock_Music as
      (
          select * from Total_Traffic
          WHERE lower(evar28) LIKE 'rock_mus%'
      )
      , Instrumental_Music as
      (
          select * from Total
          WHERE evar28 LIKE '%[ins_mus]%'
      )
      , Defined_Traffic as
      (
          select * from Rock_Music
          UNION
          select * from Instrumental_Music
      )
      select traffic_date_key
      , count(distinct visitor_id) as unique_visitor
      from Defined_Traffic
      group by traffic_date_key",
  "expected" : "2016-03-16 123"
}
*/
</code></pre>

<p>The file that contains SQL test queries is conventionally named with <code>.test</code> extension.
However, the file can be a text file with any name.
As you can see, the benefits of &ldquo;Level 2&rdquo; is retained: the supporting Java code and the actual SQL test queries are partitioned into separate files.
Each test query has a name (that tells its purpose) associated with it: key string in <code>.properties</code> file and value of &ldquo;name&rdquo; key in <code>.test</code> file.</p>

<p>In addition to those retained benefits, the most obvious benefit of this new approach is that the supporting Java code is minimal since all TestNG assertions have been removed.
The TestNG assertions, which are ubiquitous in previous &ldquo;Level 1&rdquo; and &ldquo;Level 2&rdquo; approaches, are no longer present.
Instead, the expected outputs are specified in <code>.test</code> file, in the same JSON block with each SQL query.
The whole TestNG class will only contain code to initialize a connection to database and an instance of <a href="/blog/2016/03/28/sql-unit-test-runner/">SQL Test Runner</a>, all of which is <strong>one-time setup</strong>.
As we continue writing functional tests, we can keep all tests in a single <code>.test</code> file or, optionally, group related tests into separate <code>.test</code> files.
If we add more <code>.test</code> files, we can just specify the path to the files in functions annotated with <code>@Test</code>, as shown above.</p>

<p>The main advantage of this test framework is readability of those tests, as shown in <code>.test</code> file.
The expected outputs of the SQL queries are specified in the same place, making the tests' intentions more obvious.
In the example above, the first test query&rsquo;s intention is clearer with assertion in the same location.
In addition, compared with <code>.properties</code> file approach, the SQL query is now easier to read, due to <strong>line breaks</strong>, as shown in the second example.</p>

<p>All test automation (in Java) is abstracted from data analysts, and they can read and possibly add tests totally in SQL.
Different from usual software engineering projects, in Big Data projects, data analysts (i.e., users) know more about the data than typical quality engineers.
Being able to get their input is essential in ensuring Big Data projects doing the right thing in the right ways.
If they are able to read unit test scripts and confirm the expectations, QEs will save lots of time of translating business requirements to SQL tests.</p>

<p>While it is true that we have additional computational time due to additional layers of abstraction in Java, it is minimal compared to the time to run those queries in databases.
Even then, the additional computational time is totally justified with much better readability.
Test readability will save (lots of) QE&rsquo;s time in both developing and maintaining tests, and engineer&rsquo;s time is million times more costly (per-hour-wise) than computer time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[(Pt. 0) SQL Testing in Data Marts]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/12/01/misc-sql-testing-in-data-marts/"/>
    <updated>2016-12-01T22:50:37-08:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/12/01/misc-sql-testing-in-data-marts</id>
    <content type="html"><![CDATA[<p>Data mart is a smaller version of a data warehouse, help driving business decisions of a department in a large company.
The journey of automated testing in data mart as well as other Big Data projects is tough: most of the business logics are implemented in SQL scripts.
We don&rsquo;t even know how to test data marts' functionality from the beginning: how do we know if the SQL script works or if data is correct.
Even worse, we don&rsquo;t know what defines &ldquo;unit testing&rdquo; for SQL scripts and could not enforce it on data engineers and scientists (the developers).
The fact that most of data engineers and data analysts in my organization are more comfortable with SQL, not other languages like Java or Python, is another challenge in moving toward unit testing.</p>

<p>We need an automation framework so that data engineers and quality engineers can start creating automated unit tests, instead of depending on data analysts to verify data marts manually.
I recently gave a talk on the unit test framework that allows data engineers to verify their ETL scripts in SQL, their language of choice.
I recap some of the key ideas and motivations when designing and implementing that test framework in a few blog posts.</p>

<!--
Note that SQL scripts is only a small part of ETL processes. There are other scripts such as bash, python scripts, Java programs, and/or commerical tools such as Tidal that move data and execute those SQL scripts.
-->


<h3>Functional testing for Data Mart projects</h3>

<p>We gradually figured out automated functional testing first and continued to improve it.
This <a href="/blog/2016/03/20/sql-unit-functional-tests/">blog post</a> documents the journey of automated functional testing, the rationale after each cycle of its evolution.</p>

<h3>SQL Test Runner</h3>

<p>In the latest iteration of automated testing, supporting Java code is abstracted into a SQL Test Runner.
Quality engineers and data analysts can now write test queries and assertions in more readable test blocks, using mostly SQL.
This <a href="/blog/2016/03/28/sql-unit-test-runner/">blog post</a> provides some examples.
As you can see, many design and impelmentation decisions are based on my motto: <strong>prioritize test readability</strong> when it makes sense.</p>

<h3>Incremental data update</h3>

<p>One of biggest challenges in SQL testing is &ldquo;incremental data update&rdquo; in ETL scripts.
Challenges in functional testing those scripts motivates me to ajdust the above test framework (SQL Test Runner) to allow adding unit-like tests for those ETL scripts.
This <a href="/blog/2016/04/10/sql-unit-incremental-data-update/">blog post</a> discusses &ldquo;incremental data update&rdquo; and how it should be tested as a number of unit tests.</p>

<h3>Unit testing</h3>

<p>This <a href="http://localhost:4000/blog/2016/04/12/sql-unit-testing/">blog post</a> goes into details of SQL Unit Testing.
Keep in mind that the main users of this test framework are data engineers (developers) and data analysts (testers).
Not all data engineers and analysts are comfortable with writing tests in Java or Python.
This SQL Test framework will allow them to write their tests in SQL, by hiding all Java automation details.</p>

<p>It should be noted that data analysts are really critical in validating data warehouses/datamarts.
They are the most direct users and no one understands the data better than analysts.
If the data analysts are able to read unit test scripts and confirm the expectation, quality engineers will save lots of time translating business requirements into SQL/Java tests.</p>

<h3>Functional tests vs Unit tests</h3>

<p>This <a href="/blog/2016/04/14/sql-unit-vs-functional/">blog post</a> recap the above sections by highlighting the difference between these groups of tests in the context of Big Data projects.
Note that the two groups complement each other in assuring quality and functionality of Big Data projects.</p>

<h3>Extending SQL Test Runner</h3>

<p>SQL testing for ETL process is a pretty new area to us.
Therefore, while the current SQL Unit Test framework appears adequate for most testing now, it must be able to support any new testing needs should they arise.
This <a href="/blog/2016/04/16/sql-unit-extension/">blog post</a> explains how to extend the test framework to add new functionality or features.
The SQL Unit Test framework is designed based on <a href="https://en.wikipedia.org/wiki/Open/closed_principle">Open/Closed principle</a>, and uses design patterns like Template Method and Strategy to make it easy to add new functionality.
For illustration, I will discuss how I recently added a new functionality to handle a <a href="/blog/2016/04/17/sql-unit-data-parity/">new kind of tests</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tutorial: Test-Driven Development]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/01/01/tutorial-test-driven-development/"/>
    <updated>2016-01-01T21:57:00-08:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/01/01/tutorial-test-driven-development</id>
    <content type="html"><![CDATA[<!-- Reference:
Evernote: "QE cheat sheet"
-->


<p>Test-driven development has become more and more important.
As pointed out in <a href="/syllabus/">this</a>, candidates with 3+ years of industry experience should be able to demonstrate testing experience.
You should expect testing questions come up, even when you interview for developer positions.</p>

<!--more-->


<h3>Type of tests</h3>

<p>This is useful for the open questions of &ldquo;How do you test X?&rdquo; type.
X can be anything that are not related to software.
It would be awesome if you can come up with a creative test case that interviewers won&rsquo;t think of.
However, you are usually expected to hit broadly and methodically different categories of tests, instead of keeping listing different test cases of a test category.
Knowing these kind of tests can also help you keep talking without running out of ideas.</p>

<ul>
<li>Unit testing</li>
<li>Integration testing</li>
<li>Functional testing</li>
<li>Load testing</li>
<li>Stress testing</li>
<li>Performance testing</li>
<li>Install/Uninstall testing</li>
<li>UI testing</li>
<li>Localization testing (language, market-ready)</li>
<li>Security testing</li>
<li>Acceptance testing</li>
<li>Regression testing</li>
</ul>


<p>Other general types:</p>

<ul>
<li>White-box testing</li>
<li>Black-box testing</li>
<li>Beta testing</li>
</ul>


<p>Questions that I will ask myself if I am the interviewer.</p>

<ul>
<li>Does the candidate ask good questions to understand the feature or he/she just simply making assumptions?</li>
<li>Is the candidate covering negative and boundary conditions?</li>
<li>Is the candidate trying to break the feature? Any interesting test cases you havenâ€™t thought of?</li>
<li>Is the candidate covering non-functional tests such as performance, scalability, security, etc.?</li>
</ul>


<p>Make sure that you pass all those minimum expections: ask clarifying questions and highlight assumptions.</p>

<h3>Example of a testing question</h3>

<p>For example, the interviewer will give you a question: &ldquo;Given a string, reverse it word by word&rdquo;.
You proceed to solve it and write it on the whiteboard.
The follow-up question would be &ldquo;How would you test it?&rdquo;.</p>

<p>(1) General</p>

<ul>
<li>&ldquo;Hello World&rdquo; => &ldquo;World Hello&rdquo;</li>
<li>&ldquo;Foo Bar Baz&rdquo; => &ldquo;Baz Bar Foo&rdquo;</li>
<li>What happens with unicode?</li>
<li>What about tabs and newlines? Do newlines end up in the beginning or end?</li>
<li>What about unicode whitespace?</li>
<li>What about numbers? What about special characters like ! and #.</li>
</ul>


<p>(2) Boundary</p>

<ul>
<li>Null string: null => what happens?</li>
<li>Empty string: &ldquo;&rdquo; => &ldquo;&rdquo;</li>
<li>Really long string => What happens?</li>
<li>What about a one really really long word?</li>
<li>What if you just give a string of spaces <code>"     "</code> => <code>"     "</code>?</li>
<li>What if you give a really string of letters separated by spaces? <code>"a a a a a a a a a a a a a a a a a a aa a"</code></li>
<li>Does it handle Chinese well?</li>
<li>What if there&rsquo;s multiple spaces? Are they respected? <code>"a a a b ob"</code></li>
</ul>


<p>(3) Exceptional</p>

<ul>
<li>What happens when you give a really long string (like from <code>/dev/random</code>)?</li>
<li>How can the function run out of memory?</li>
<li>What if the type is incorrect? Could happen in Python.</li>
<li>null => depends on specification. Might not be possible in languages like Haskell.</li>
</ul>


<p>Four and five are bonus stuff that I like to cover or have been asked of me in the past.
They generally aren&rsquo;t the first three that I think of when I&rsquo;m asked to test a function though.</p>

<p>(4) Performance - Running time</p>

<p>I would explain the space complexity I expect out of my solution.
I would draw a simple graph of what it would probably look like.
I would then provide a set of sample points that I would take to see the space complexity.
Some sample points I would take are 0, 1, 5, 10, <code>100</code>, <code>1,000</code>, <code>10,000</code>, <code>100,000</code>, <code>1,000,000</code> until I hit some barrier and then draw more fine grain values in between.
Basically, I&rsquo;d increase the input exponentially and then increase granularity from there.</p>

<p>(5) Performancce - Space complexity</p>

<p>Very similar to (4), I would explain the space complexity I expect out of my solution.
I would draw a simple graph of what it would probably look like.
I would then provide a set of sample points that I would take to see the space complexity.
Personally, I would mention that many languages like Java allow you to run at a reduced heap size like 4 MB so that you can hit <code>OutOfMemory</code> exceptions very quickly with reasonably sized inputs, if the complexity isn&rsquo;t managed properly.</p>
]]></content>
  </entry>
  
</feed>
