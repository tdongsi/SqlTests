<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Concurrency | Personal Interview Notes]]></title>
  <link href="http://tdongsi.github.io/SqlTests/blog/categories/concurrency/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/SqlTests/"/>
  <updated>2021-10-23T19:56:22-07:00</updated>
  <id>http://tdongsi.github.io/SqlTests/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tutorial: Process Synchronization]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/06/02/tutorial-process-synchronization/"/>
    <updated>2016-06-02T00:35:03-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/06/02/tutorial-process-synchronization</id>
    <content type="html"><![CDATA[<p>Summary of chapter 5 of &ldquo;Operating System concepts&rdquo; (Dinosaur book).
Topics in this chapter are the most intensive and frequently asked during interviews.</p>

<!--more-->


<p>This chapter discuss how to prevent concurrent access to shared data that may result in data inconsistency.</p>

<h3>5.1 &amp; 5.2: Critical section</h3>

<p>Consider the producer–consumer problem, which is representative of
operating systems. Specifically, in Section 3.4.1, we described how a bounded
buffer could be used to enable processes to share memory.</p>

<pre><code class="cpp Producer process">while (true ) {

  /* produce an item in next produced */
  while (counter == BUFFER SIZE )
    ; /* do nothing */

  buffer[in] = next produced;
  in = (in + 1) % BUFFER SIZE ;
  counter++;
}
</code></pre>

<pre><code class="cpp Consumer process">while (true ) {
  while (counter == 0)
    ; /* do nothing */

  next consumed = buffer[out];
  out = (out + 1) % BUFFER SIZE ;
  counter--;
  /* consume the item in next consumed */
}
</code></pre>

<p>The above code works incorrectly in multi-thread or process situation, due to parallel modifications to &ldquo;counter&rdquo;.
When several processes access and manipulate the same data concurrently and the
outcome of the execution depends on the particular order in which the access
takes place, is called a race condition.</p>

<p>Critical section: No two processes are executing in their critical sections at the same time.
Solution to a critical section problem requires:
Mutual exclusion: only one process in CS at a time.
Progress: Selection process should not be postponed indefinitely.
Bounded waiting: once a process request to enter, waiting time is bounded.</p>

<p>Nonpreemptive kernel does not allow a process running in kernel mode to be
preempted; a kernel-mode process will run until it exits kernel mode, blocks,
or voluntarily yields control of the CPU. A nonpreemptive kernel is essentially free from race conditions.
A preemptive kernel may be more responsive, since there is less risk that a
kernel-mode process will run for an arbitrarily long period.</p>

<h3>5.3: Peterson&rsquo;s algorithm</h3>

<pre><code class="cpp Peterson's algorithm">flag[i] = true
turn = j

while ( flag[j] &amp;&amp; turn == j ) {
}

// start of CS

// end of CS
flag[i] = false flag[j] = true
turn = i

while( flag[i] &amp;&amp; turn == i ) {
}

// start of CS

// end of CS
flag[j] = false
</code></pre>

<p>NOTE: <a href="http://en.wikipedia.org/wiki/Peterson's_algorithm">Peterson’s algorithm</a> is restricted to two processes.
Filter algorithm: Peterson&rsquo;s algorithm for N processes
The filter algorithm generalizes Peterson&rsquo;s algorithm for N processes. It uses N different levels - each represents another &lsquo;waiting room&rsquo;, before the critical section. Each level will allow at least one process to advance, while keeping one process in waiting.</p>

<p><a href="http://cs.stackexchange.com/questions/12621/understanding-peterson-s-and-dekker-s-algorithms">http://cs.stackexchange.com/questions/12621/understanding-peterson-s-and-dekker-s-algorithms</a>
Both processes indicates if the other want to enter CS, it can proceed. If both processes enter at the same time, turn will be set to i and j at the same time, and only one will last.
Proof of bounded waiting: Pi will enter the CS after at most one entry by Pj.</p>

<pre><code class="plain Analogies of Peterson's algorithm">Peterson's: "I want to enter."                 flag[0]=true;
            "You can enter next."              turn=1;
            "If you want to enter and          while(flag[1]==true&amp;&amp;turn==1){
            it's your turn I'll wait."         }
            Else: Enter CS!                    // CS
            "I don't want to enter any more."  flag[0]=false;

Dekker's:   "I want to enter."                 flag[0]=true;
            "If you want to enter              while(flag[1]==true){
             and if it's your turn               if(turn!=0){
             I don't want to enter any more."      flag[0]=false;
            "If it's your turn                     while(turn!=0){
             I'll wait."                           }
            "I want to enter."                     flag[0]=true;
                                                 }
                                               }
            Enter CS!                          // CS
            "You can enter next."              turn=1;
            "I don't want to enter any more."  flag[0]=false;
</code></pre>

<h3>5.4: Sync using hardware</h3>

<p>Protect critical section by locking.</p>

<p>Many modern computer systems therefore provide special hardware
instructions that allow us either to test and modify the content of a word or
to swap the contents of two words atomically—that is, as one uninterruptible unit.</p>

<p>Atomic test_and_set() and compare_and_swap() for locking:</p>

<pre><code class="c">boolean test_and_set(boolean *target) {
  boolean rv = *target;
  *target = true;
  return rv;
}

int compare_and_swap(int *value, int expected, int new value) {
  int temp = *value;
  if (*value == expected)
      *value = new value;
  return temp;
}
</code></pre>

<p>Simple Mutex with atomic test_and_set()
Figure 5.5</p>

<p>Bounded-Waiting mutex with atomic test_and_set(): data structure and algorithm
Figure 5.7</p>

<h3>5.5: Mutex locks</h3>

<p>We use the mutex to lock to protect critical regions and thus prevent race conditions.</p>

<p>Calls to either acquire() or release() must be performed atomically.</p>

<pre><code class="c">acquire() {
  while (!available)
    ; /* busy wait */
  available = false;
}

release() {
  available = true;
}
</code></pre>

<p>Usage:</p>

<pre><code class="c">acquire()

// start of CS

// end of CS

release()
</code></pre>

<p>The main disadvantage of the implementation given here is that it requires busy waiting. This type of mutex lock is also called a spinlock.
Spinlocks do have an advantage, however, in that no context switch is required.</p>

<h3>5.6: Semaphores</h3>

<p>A semaphore S is an integer variable that, apart from initialization, is
accessed only through two standard atomic operations: wait() and signal().</p>

<pre><code class="c">wait(S) {
  while (S &lt;= 0 )
    ; // busy wait
  S--;
}

signal(S) {
  S++;
}
</code></pre>

<p>The value of a counting semaphore can range over an unrestricted domain. Versus binary semaphore, which is similar to mutex.
Counting semaphores can be used to control access to a given resources of a finite number of instances.</p>

<p>We can also use semaphores to solve various synchronization problems.
For example,consider two concurrently running processes: P1 with a statement
S1 and P2 with a statement S2 . Suppose we require that S2 be executed only
after S1 has completed. We can implement this scheme readily by letting P1
and P2 share a common semaphore synch, initialized to 0. In process P1 , we
insert the statements</p>

<pre><code class="c">S1;
signal(synch);
</code></pre>

<p>In process P2 , we insert the statements</p>

<pre><code class="c">wait(synch);
S2;
</code></pre>

<p>Because synch is initialized to 0, P2 will execute S2 only after P1 has invoked
signal(synch) , which is after statement S1 has been executed.</p>

<p>Deadlock:</p>

<pre><code>P 0 P 1
wait(S); wait(Q);
wait(Q); wait(S);
. .
. .
. .
signal(S); signal(Q);
signal(Q); signal(S);
</code></pre>

<p>Priority inversion:</p>

<p>The problem of priority inversion is when three processes of different priorities L &lt; M &lt; H. H is waiting for L to finish with a certain resource. M process becomes runnable and preempts L. Indirectly, process M with lower priority affects how long process H must wait for resource.
It occurs when the system has more than two priorities. However, it is almost always the case.</p>

<p>Solution: priority-inheritance protocol: all processes that use a resource, waited by a higher priority process, will inherit the highest priority until they are done with the resource.</p>

<h4>Semaphore implementation</h4>

<p>The naive definition of wait() and signal() above presents the same problem of busy waiting.
In actual implementation, when a process execute wait() operation and find that semaphore value is not positive, it must wait. However, instead of busy waiting, the process block itself.
In this implementation, semaphore values may be negative, while they are never negative in classical definition with busy waiting. If a semaphore value is negative, its magnitude indicates the number of waiting processes (note different order of decrement in wait()).</p>

<p>It is critical that semaphore operations be executed atomically: no two processes can execute wait() and signal() operations on the same semaphore at the same time.
In a multiprocessor environment, usually compare_and_swap() or spin locks are used to ensure wait() and signal() are atomic.
So, we admit that busy waiting is NOT eliminated in this implementation. However, busy waiting is limited to CS of the wait() and signal() operations. These CSs are short (about 10 instructions). Thus, CS is almost never occupied, and busy waiting is rare and short, if ever happens.</p>

<h3>5.7: Classic Problems of Synchronization</h3>

<p>Use semaphores for synchronization. Actual implementation can use mutex instead of binary semaphore.</p>

<p>Bounded-Buffer (Consumer-Producer) problem
Problem: See 5.1.
Solution: The producer and consumer share the following data structure:
The mutex is used to provide mutual exclusion for accesses to the buffer pool.</p>

<pre><code class="c">int n;
semaphore mutex = 1;
semaphore empty = n;
semaphore full = 0

// Producer
do {
. . .
/* produce an item in next produced */
. . .
wait(empty);
wait(mutex);
. . .
/* add next produced to the buffer */
. . .
signal(mutex);
signal(full);
} while (true);

// Consumer
do {
wait(full);
wait(mutex);
. . .
/* remove an item from buffer to next consumed */
. . .
signal(mutex);
signal(empty);
. . .
/* consume the item in next consumed */
. . .
} while (true);
</code></pre>

<p>Reader-Writer problem:
Writers should have exclusive access while writing to the shared database.
There are several variants of “reader-writer” problems:
First problem: No reader to be kept waiting unless a writer has already obtained accesss.
Second problem: Once writer is ready, that writer perform its write ASAP. No new readers may start reading.
A solution to either problem may result in starvation.</p>

<p>Solution to first reader-writer problem: shared data structure
semaphore rw_utex = 1;
semaphore mutex = 1;
int read count = 0;
The mutex semaphore is used to ensure mutual exclusion when the variable read count is updated.
The read count variable keeps track of how many processes are currently reading the object.
The semaphore rw_mutex functions as a mutual exclusion semaphore for the writers.</p>

<pre><code class="c">// Writer   
do {
wait(rw mutex);
. . .
/* writing is performed */
. . .
signal(rw mutex);
} while (true);

// Reader
do {
wait(mutex);
read count++;
if (read count == 1)
wait(rw mutex);
signal(mutex);
. . .
/* reading is performed */
. . .
wait(mutex);
read count--;
if (read count == 0)
signal(rw mutex);
signal(mutex);
} while (true);
</code></pre>

<p>Dining Philosopher problem: This solution can create a deadlock</p>

<pre><code class="c">semaphore chopstick[5];

do {
wait(chopstick[i]);
wait(chopstick[(i+1) % 5]);
. . .
/* eat for awhile */
. . .
signal(chopstick[i]);
signal(chopstick[(i+1) % 5]);
. . .
/* think for awhile */
. . .
} while (true);
</code></pre>

<p>Several possible remedies to the deadlock problem are replaced by:</p>

<ul>
<li>Allow at most four philosophers to be sitting simultaneously at the table.</li>
<li>Allow a philosopher to pick up her chopsticks only if both chopsticks are
available (to do this, she must pick them up in a critical section).</li>
<li>Use an asymmetric solution—that is,an odd-numbered philosopher picks
up first her left chopstick and then her right chopstick, whereas an even-
numbered philosopher picks up her right chopstick and then her left
chopstick.</li>
</ul>


<p>In 5.8, we use monitor (equivalent to a waiter to tell which philosopher should eat) to provides deadlock-free solution.
A deadlock-free solution does not necessarily eliminate the possibility of starvation.</p>

<h3>5.8: Monitors</h3>

<p>Semaphore is not a complete solution. If a single process is not well-behaved (semaphore used incorrectly), the system break down.
Incorrect order of signal() and wait(): mutual exclusion is no longer guaranteed.
wait() is used in place of signal(): a deadlock may occur.
wait() or signal() or both are omitted: mutual exclusion violated or deadlock.</p>

<p>Syntax of a monitor:</p>

<p>Local variables of a monitor can be accessed by only the local functions. Only one process at a time is active within the monitor.
We also defines condition construct: condition x, y; // condition variables
The only operations that can be invoked on a condition variable are wait() and signal().
The operation x.wait(); means that the process invoking this operation is suspended until another process invokes x.signal();
The x.signal() operation resumes exactly one suspended process. If no process is suspended, then the signal() operation has no effect.
(Different from semaphore’s signal(): semaphore() signal always change the state of semaphore, condition’s signal() may not).</p>

<p>Dining Philosophers solution using Monitors:
Monitor is acting like a waiter/moderator. Before a philosopher starts eating, she informs the waiter (invoked operation pickup()) and the waiter will tell her what to do.
After she is done eating, she again informs the waiter (putdown()). It is still possible that some philosopher will starve to death.</p>

<p>Resuming Processes within a Monitor
One simple solution is to use FIFO ordering.
Another solution is conditional-wait construct, with c is the priority number input.
x.wait&copy;;
When x.signal() is executed, the process with the smallest priority number is resumed next.</p>

<p>Java monitors
Java uses monitor for thread synchronization.
Every object in Java has a single lock associated with it. When a method is declared synchronized, calling the method requires owning the lock of the object.
If the lock is not available, the synchronized method is placed in the entry set for the object’s lock.
The Java Object class’s method wait() and notify() are similar to wait() and signal() statements for a monitor.</p>

<h3>Reference</h3>

<ul>
<li><a href="/download/Java_Concurrency.pdf">Java Concurrency</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tutorial: Concurrent Cache]]></title>
    <link href="http://tdongsi.github.io/SqlTests/blog/2016/04/06/tutorial-concurrent-cache/"/>
    <updated>2016-04-06T21:08:39-07:00</updated>
    <id>http://tdongsi.github.io/SqlTests/blog/2016/04/06/tutorial-concurrent-cache</id>
    <content type="html"><![CDATA[<p>One common interview question is &ldquo;How to design/implement a Concurrent Cache (or Concurrent HashMap)?&rdquo;.
After reading &ldquo;Go Programming Language&rdquo; book, this problem can be approached in a methodical way, especially in Go.</p>

<!--more-->


<h3>Summary</h3>

<ul>
<li>Naive answer: putting a HashMap in a Critical Section, guarded by a Mutex. It will be blocking for anyone who tries to access it.</li>
<li>Better: Separate locking/unlocking for first entry and repeated entry.</li>
<li>Expected: Duplicate supression.</li>
</ul>


<h3>Naive design</h3>

<pre><code class="go">// A Memo caches the results of calling a Func.
type Memo struct {
    f Func
    mu sync.Mutex // guards cache
    cache map[string]result
}

// Func is the type of the function to memoize.
type Func func(key string) (interface{}, error)

type result struct {
    value interface{}
    err error
}

func New(f Func) *Memo {
    return &amp;Memo {
        f: f,
        cache: make(map[string]result),
    }
}

func (memo *Memo) Get(key string) (interface{}, error) {
    memo.mu.Lock()
    res, ok := memo.cache[key]
    if !ok {
        res.value, res.err = memo.f(key)
        memo.cache[key] = res
    }
    memo.mu.Unlock()
    return res.value, res.err
}
</code></pre>

<h3>Better: Separate locking/unlocking for repeated entries</h3>

<pre><code class="go">// A Memo caches the results of calling a Func.
type Memo struct {
    f Func
    mu sync.Mutex // guards cache
    cache map[string]result
}

// Func is the type of the function to memoize.
type Func func(key string) (interface{}, error)

type result struct {
    value interface{}
    err error
}

func New(f Func) *Memo {
    return &amp;Memo {
        f: f,
        cache: make(map[string]result),
    }
}

func (memo *Memo) Get(key string) (interface{}, error) {
    memo.mu.Lock()
    res, ok := memo.cache[key]
    memo.mu.Unlock()

    if !ok {
        res.value, res.err = memo.f(key)

        memo.mu.Lock()
        memo.cache[key] = res
        memo.mu.Unlock()
    }

    return res.value, res.err
}
</code></pre>

<h3>Expected: Duplicate-suppressing</h3>

<pre><code class="go">// A Memo caches the results of calling a Func.
type Memo struct {
    f Func
    mu sync.Mutex // guards cache
    cache map[string]*entry
}

// Func is the type of the function to memoize.
type Func func(key string) (interface{}, error)

type entry struct {
    res result
    ready chan struct{}  // closed when res is ready
}

type result struct {
    value interface{}
    err error
}

func New(f Func) *Memo {
    return &amp;Memo {
        f: f,
        cache: make(map[string]*entry),
    }
}

func (memo *Memo) Get(key string) (interface{}, error) {
    memo.mu.Lock()
    e := memo.cache[key]
    if e == nil {
        // first request
        e = &amp;entry{ready: make(chan struct{})}
        memo.cache[key] = e
        memo.mu.Unlock()

        e.res.value, e.res.err = memo.f(key)

        close(e.ready) // broadcast ready condition
    } else {
        // repeat request
        memo.mu.Unlock()

        &lt;- e.ready // wait for ready condition
    }

    return e.res.value, e.res.err
}
</code></pre>
]]></content>
  </entry>
  
</feed>
